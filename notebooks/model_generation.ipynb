{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "pip install keras_nlp"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras_nlp\n",
            "  Downloading keras_nlp-0.3.0-py3-none-any.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 12.2 MB/s \n",
            "\u001b[?25hCollecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 45.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras_nlp) (1.21.6)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras_nlp) (2.9.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras_nlp) (21.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from keras_nlp) (1.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras_nlp) (3.0.9)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (0.4.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (2.9.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (1.6.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (0.27.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (2.9.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (3.17.3)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (2.9.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (1.12)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (4.1.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (14.0.6)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (1.50.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (3.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (2.0.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (1.14.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (3.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras_nlp) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras_nlp) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (1.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (3.2.2)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->keras_nlp) (0.12.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 578.0 MB 15 kB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of tensorflow-text to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.6 MB 49.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: tensorflow-text, keras-nlp\n",
            "Successfully installed keras-nlp-0.3.0 tensorflow-text-2.9.0\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TnI4uAbrLx7",
        "outputId": "8689687e-4293-451c-eb1f-7be61c0c232a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "import keras_nlp\r\n",
        "import numpy as np\r\n",
        "import os"
      ],
      "outputs": [],
      "metadata": {
        "id": "kevRkb9aqXAd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "texts = \"\"\r\n",
        "\r\n",
        "for(dirpath, dirnames, filenames) in os.walk(\"poems/\"):\r\n",
        "  for filename in filenames:\r\n",
        "    if filename.endswith('.txt'):\r\n",
        "        # print(filename)\r\n",
        "        path = os.path.join(dirpath, filename)\r\n",
        "        with open(path,'r') as file_stream:\r\n",
        "            texts += file_stream.read()\r\n",
        "print(texts[0:1000])\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.gutenberg.org/files/2554/2554-0.txt\n",
            "1201520/1201520 [==============================] - 0s 0us/step\n",
            "Downloading data from https://www.gutenberg.org/files/28054/28054-0.txt\n",
            "2041953/2041953 [==============================] - 0s 0us/step\n",
            "Downloading data from https://www.gutenberg.org/files/2638/2638-0.txt\n",
            "1427675/1427675 [==============================] - 0s 0us/step\n",
            "Downloading data from https://www.gutenberg.org/files/8117/8117-0.txt\n",
            "1483181/1483181 [==============================] - 0s 0us/step\n",
            " for some unknown\n",
            "reason, was being taken somewhere in a huge waggon dragged by a heavy\n",
            "dray horse, suddenly shouted at him as he drove past: “Hey there, German\n",
            "hatter” bawling at the top of his voice and pointing at him--the young\n",
            "man stopped suddenly and clutched tremulously at his hat. It was a tall\n",
            "round hat from Zimmerman’s, but completely worn out, rusty with age, all\n",
            "torn and bespattered, brimless and bent on one side in a most unseemly\n",
            "fashion. Not shame, however, but quite another feeling akin to terror\n",
            "had overtaken him.\n",
            "\n",
            "“I knew it,” he muttered in confusion, “I thought so! That’s the worst\n",
            "of all! Why, a stupid thing like this, the most trivial detail might\n",
            "spoil the whole plan. Yes, my hat is too noticeable.... It looks absurd\n",
            "and that makes it noticeable.... With my rags I ought to wear a cap, any\n",
            "sort of old pancake, but not this grotesque thing. Nobody wears such\n",
            "a hat, it would be noticed a mile off, it would be remembered.... What\n",
            "matters is that people would remember it, and that would give them\n",
            "a clue. For this business one should be as little conspicuous as\n",
            "possible.... Trifles, trifles are what matter! Why, it’s just such\n",
            "trifles that always ruin everything....”\n",
            "\n",
            "He had not far to go; he knew indeed how many steps it was from the gate\n",
            "of his lodging house: exactly seven hundred and thirty. He had counted\n",
            "them once when he had been lost in dreams. At the time he had put no\n",
            "faith in those dreams and was only tantalising himself by their hideous\n",
            "but daring recklessness. Now, a month later, he had begun to look upon\n",
            "them differently, and, in spite of the monologues in which he jeered at\n",
            "his own impotence and indecision, he had involuntarily come to regard\n",
            "this “hideous” dream as an exploit to be attempted, although he\n",
            "still did not realise this himself. He was positively going now for a\n",
            "“rehearsal” of his project, and at every step his excitement grew more\n",
            "and more violent.\n",
            "\n",
            "With a sinking heart and a nervous tremor, he went up to a huge house\n",
            "which on one side looked on to the canal, and on the other into the\n",
            "street. This house was let out in tiny tenements and was inhabited by\n",
            "working people of all kinds--tailors, locksmiths, cooks, Germans of\n",
            "sorts, girls picking up a living as best they could, petty clerks, etc.\n",
            "There was a continual coming and going through the two gates and in the\n",
            "two courtyards of the house. Three or four door-keepers were employed on\n",
            "the building. The young man was very glad to meet none of them, and\n",
            "at once slipped unnoticed through the door on the right, and up the\n",
            "staircase. It was a back staircase, dark and narrow, but he was familiar\n",
            "with it already, and knew his way, and he liked all these surroundings:\n",
            "in such darkness even the most inquisitive eyes were not to be dreaded.\n",
            "\n",
            "“If I am so scared now, what would it be if it somehow came to pass that\n",
            "I were really going to do it?” he could not help asking himself as he\n",
            "reached the fourth storey. There his progress was barred by some porters\n",
            "who were engaged in moving furniture out of a flat. He knew that the\n",
            "flat had been occupied by a German clerk in the civil service, and his\n",
            "family. This German was moving out then, and so the fourth floor on this\n",
            "staircase would be untenanted except by the old woman. “That’s a good\n",
            "thing anyway,” he thought to himself, as he rang the bell of the old\n",
            "woman’s flat. The bell gave a faint tinkle as though it were made of\n",
            "tin and not of copper. The little flats in such houses always have bells\n",
            "that ring like that. He had forgotten the note of that bell, and now\n",
            "its peculiar tinkle seemed to remind him of something and to bring it\n",
            "clearly before him.... He started, his nerves were terribly overstrained\n",
            "by now. In a little while, the door was opened a tiny crack: the old\n",
            "woman eyed her visitor with evident distrust through the crack, and\n",
            "nothing could be seen but her little eyes, glittering in the darkness.\n",
            "But, seeing a number of people on the landing, she grew bolder, and\n",
            "opened the door wide. The young man stepped into the dark entry, which\n",
            "was partitioned off from the tiny kitchen. The old woman stood facing\n",
            "him in silence and looking inquiringly at him. She was a diminutive,\n",
            "withered up old woman of sixty, with sharp malignant eyes and a sharp\n",
            "little nose. Her colourless, somewhat grizzled hair was thickly smeared\n",
            "with oil, and she wore no kerchief over it. Round her thin long neck,\n",
            "which looked like a hen’s leg, was knotted some sort of flannel rag,\n",
            "and, in spite of the heat, there hung flapping on her shoulders, a mangy\n",
            "fur cape, yellow with age. The old woman coughed and groaned at every\n",
            "instant. The young man must have looked at her with a rather peculiar\n",
            "expression, for a gleam of mistrust came into her eyes again.\n",
            "\n",
            "“Raskolnikov, a student, I came here a month ago,” the young man made\n",
            "haste to mutter, with a half bow, remembering that he ought to be more\n",
            "polite.\n",
            "\n",
            "“I remember, my good sir, I remember quite well your coming here,” the\n",
            "old woman said distinctly, still keeping \n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03WXT5hBrzM7",
        "outputId": "be8b647e-41b9-4114-992f-3446fe6624e3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "def invert_sentences(texts):\r\n",
        "  texts = [\" \".join(reversed(text.split(' '))) for text in texts] \r\n",
        "  return texts\r\n",
        "\r\n",
        "text_list = texts.split('.')\r\n",
        "text_list = list(filter(None, text_list))\r\n",
        "\r\n",
        "text_list = invert_sentences(text_list)\r\n",
        "\r\n",
        "import random\r\n",
        "#random.shuffle(text_list)\r\n",
        "length = len(text_list)\r\n",
        "#split the dataset \r\n",
        "text_train = text_list[:int(0.7*length)]\r\n",
        "text_test = text_list[int(0.7*length):int(0.85*length)]\r\n",
        "text_valid = text_list[int(0.85*length):]"
      ],
      "outputs": [],
      "metadata": {
        "id": "H9nsqeFzsEaX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "from tensorflow.keras.layers import TextVectorization\r\n",
        "\r\n",
        "#def custom_standardization(input_string):\r\n",
        "#    sentence = tf.strings.lower(input_string)\r\n",
        "#    sentence = tf.strings.regex_replace(sentence, \"\\n\", \" \")\r\n",
        "#    return sentence\r\n",
        "\r\n",
        "maxlen = 100\r\n",
        "# You can also set calculate the longest sentence in the data - 25 in this case\r\n",
        "#maxlen = len(max(text_list).split(' ')) \r\n",
        "\r\n",
        "vectorize_layer = TextVectorization(\r\n",
        "#    standardize = custom_standardization,\r\n",
        "    output_mode=\"int\",\r\n",
        "    output_sequence_length=maxlen + 1,\r\n",
        ")\r\n",
        "\r\n",
        "vectorize_layer.adapt(text_list)\r\n",
        "vocab = vectorize_layer.get_vocabulary()\r\n",
        "vocab_size = len(vocab)\r\n",
        "index_lookup = dict(zip(range(len(vocab)), vocab))   \r\n",
        "#example vectorrizing words\r\n",
        "print(vectorize_layer(['hello world !']))\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[  1 319   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0]], shape=(1, 101), dtype=int64)\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6lp19ohs1Eg",
        "outputId": "c80bc8aa-9069-458f-df42-add4f7dc66d7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "#DataSet creation\r\n",
        "def create_dataset(text, batch_size):\r\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(text)\r\n",
        "  dataset = dataset.shuffle(buffer_size=256)\r\n",
        "  dataset = dataset.batch(batch_size)\r\n",
        "  return dataset \r\n",
        "\r\n",
        "def preprocess_text(text):\r\n",
        "    text = tf.expand_dims(text, -1)\r\n",
        "    tokenized_sentences = vectorize_layer(text)\r\n",
        "    x = tokenized_sentences[:, :-1]\r\n",
        "    y = tokenized_sentences[:, 1:]\r\n",
        "    return x, y\r\n",
        "\r\n",
        "batch_size = 64\r\n",
        "\r\n",
        "train_dataset = create_dataset(text_train, batch_size)\r\n",
        "\r\n",
        "valid_dataset = create_dataset(text_valid, batch_size)\r\n",
        "\r\n",
        "test_dataset = create_dataset(text_test, batch_size)\r\n",
        "\r\n",
        "#preprocess text\r\n",
        "train_dataset = train_dataset.map(preprocess_text)\r\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\r\n",
        "\r\n",
        "test_dataset = test_dataset.map(preprocess_text)\r\n",
        "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\r\n",
        "\r\n",
        "valid_dataset = valid_dataset.map(preprocess_text)\r\n",
        "valid_dataset = valid_dataset.prefetch(tf.data.AUTOTUNE)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "Y2rIIKCHxJ05"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "embed_dim = 128\r\n",
        "num_heads = 4\r\n",
        "\r\n",
        "def create_model():\r\n",
        "    inputs = keras.layers.Input(shape=(maxlen,), dtype=tf.int32)\r\n",
        "    embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(vocab_size, maxlen, embed_dim)(inputs)\r\n",
        "    decoder = keras_nlp.layers.TransformerDecoder(intermediate_dim=embed_dim, \r\n",
        "                                                            num_heads=num_heads, \r\n",
        "                                                            dropout=0.5)(embedding_layer)\r\n",
        "    \r\n",
        "    outputs = keras.layers.Dense(vocab_size, activation='softmax')(decoder)\r\n",
        "    \r\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\r\n",
        "    \r\n",
        "    model.compile(\r\n",
        "        optimizer=\"adam\", \r\n",
        "        loss='sparse_categorical_crossentropy',\r\n",
        "        metrics=[keras_nlp.metrics.Perplexity(), 'accuracy']\r\n",
        "    )\r\n",
        "    return model\r\n",
        "\r\n",
        "model = create_model()\r\n",
        "model.summary()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 100)]             0         \n",
            "                                                                 \n",
            " token_and_position_embeddin  (None, 100, 128)         3868288   \n",
            " g (TokenAndPositionEmbeddin                                     \n",
            " g)                                                              \n",
            "                                                                 \n",
            " transformer_decoder (Transf  (None, 100, 128)         99584     \n",
            " ormerDecoder)                                                   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 100, 30121)        3885609   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,853,481\n",
            "Trainable params: 7,853,481\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yINsowoz_4V",
        "outputId": "a2edcabf-4854-44e7-9135-d598772cf3e8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "#Custom Callback\r\n",
        "class TextSampler(keras.callbacks.Callback):\r\n",
        "    def __init__(self, start_prompt, max_tokens):\r\n",
        "        self.start_prompt = start_prompt\r\n",
        "        self.max_tokens = max_tokens\r\n",
        "        \r\n",
        "    # Helper method to choose a word from the top K probable words with respect to their probabilities\r\n",
        "    # in a sequence\r\n",
        "    def sample_token(self, logits):\r\n",
        "        logits, indices = tf.math.top_k(logits, k=5, sorted=True)\r\n",
        "        indices = np.asarray(indices).astype(\"int32\")\r\n",
        "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\r\n",
        "        preds = np.asarray(preds).astype(\"float32\")\r\n",
        "        return np.random.choice(indices, p=preds)\r\n",
        "\r\n",
        "    def on_epoch_end(self, epoch, logs=None):\r\n",
        "        decoded_sample = self.start_prompt\r\n",
        "        \r\n",
        "        for i in range(self.max_tokens-1):\r\n",
        "            tokenized_prompt = vectorize_layer([decoded_sample])[:, :-1]\r\n",
        "            predictions = self.model.predict([tokenized_prompt], verbose=0)\r\n",
        "            # To find the index of the next word in the prediction array.\r\n",
        "            # The tokenized prompt is already shorter than the original decoded sample\r\n",
        "            # by one, len(decoded_sample.split()) is two words ahead - so we remove 1 to get\r\n",
        "            # the next word in the sequence\r\n",
        "            sample_index = len(decoded_sample.strip().split())-1\r\n",
        "            \r\n",
        "            sampled_token = self.sample_token(predictions[0][sample_index])\r\n",
        "            sampled_token = index_lookup[sampled_token]\r\n",
        "            decoded_sample += \" \" + sampled_token\r\n",
        "            \r\n",
        "        print(f\"\\nSample text:\\n{decoded_sample}...\\n\")\r\n",
        "\r\n",
        "# First 5 words of a random sentence to be used as a seed\r\n",
        "random_sentence = ' '.join(random.choice(text_valid).replace('\\n', ' ').split(' ')[:4])\r\n",
        "sampler = TextSampler(random_sentence, 30)\r\n",
        "reducelr = keras.callbacks.ReduceLROnPlateau(patience=10, monitor='val_loss')"
      ],
      "outputs": [],
      "metadata": {
        "id": "zLlH-CMp2OX-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "model = create_model()\r\n",
        "history = model.fit(train_dataset, \r\n",
        "                    validation_data=valid_dataset,\r\n",
        "                    epochs=10,\r\n",
        "                    callbacks=[sampler,reducelr])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "633/634 [============================>.] - ETA: 0s - loss: 1.5276 - perplexity: 4.6072 - accuracy: 0.8317\n",
            "Sample text:\n",
            "yet complete not is that you  for me at him with it that know not could she when  you do   you that know don’t “i ” “oh that you...\n",
            "\n",
            "634/634 [==============================] - 139s 214ms/step - loss: 1.5276 - perplexity: 4.6072 - accuracy: 0.8317 - val_loss: 1.0854 - val_perplexity: 2.9605 - val_accuracy: 0.8488 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "633/634 [============================>.] - ETA: 0s - loss: 1.0083 - perplexity: 2.7408 - accuracy: 0.8469\n",
            "Sample text:\n",
            "yet complete not is he what see i that me to going are we what and you  that you if as me told i   that say i and me at...\n",
            "\n",
            "634/634 [==============================] - 136s 215ms/step - loss: 1.0084 - perplexity: 2.7411 - accuracy: 0.8469 - val_loss: 1.0681 - val_perplexity: 2.9098 - val_accuracy: 0.8493 - lr: 0.0010\n",
            "Epoch 3/10\n",
            "633/634 [============================>.] - ETA: 0s - loss: 0.9489 - perplexity: 2.5829 - accuracy: 0.8498\n",
            "Sample text:\n",
            "yet complete not is that know i but you that you do you  do what know you  do you do what to wish you  did what know to order the...\n",
            "\n",
            "634/634 [==============================] - 136s 214ms/step - loss: 0.9489 - perplexity: 2.5829 - accuracy: 0.8498 - val_loss: 1.0703 - val_perplexity: 2.9162 - val_accuracy: 0.8506 - lr: 0.0010\n",
            "Epoch 4/10\n",
            "633/634 [============================>.] - ETA: 0s - loss: 0.9048 - perplexity: 2.4715 - accuracy: 0.8516\n",
            "Sample text:\n",
            "yet complete not is that know i but me tell you to wished she did you do what know you ” “then  to  said he that the in that and you...\n",
            "\n",
            "634/634 [==============================] - 136s 214ms/step - loss: 0.9048 - perplexity: 2.4715 - accuracy: 0.8516 - val_loss: 1.0766 - val_perplexity: 2.9346 - val_accuracy: 0.8511 - lr: 0.0010\n",
            "Epoch 5/10\n",
            "633/634 [============================>.] - ETA: 0s - loss: 0.8702 - perplexity: 2.3873 - accuracy: 0.8529\n",
            "Sample text:\n",
            "yet complete not is it is she what  and more is there that me tell to wish i but nothing said he ” “what his and the in him after  and...\n",
            "\n",
            "634/634 [==============================] - 136s 215ms/step - loss: 0.8702 - perplexity: 2.3873 - accuracy: 0.8529 - val_loss: 1.0946 - val_perplexity: 2.9879 - val_accuracy: 0.8510 - lr: 0.0010\n",
            "Epoch 6/10\n",
            "633/634 [============================>.] - ETA: 0s - loss: 0.8430 - perplexity: 2.3233 - accuracy: 0.8538\n",
            "Sample text:\n",
            "yet complete not is it that so me to you of out it put i think you  did “i  prince ” “oh in but   the in and  then...\n",
            "\n",
            "634/634 [==============================] - 136s 215ms/step - loss: 0.8430 - perplexity: 2.3233 - accuracy: 0.8538 - val_loss: 1.1131 - val_perplexity: 3.0438 - val_accuracy: 0.8511 - lr: 0.0010\n",
            "Epoch 7/10\n",
            "633/634 [============================>.] - ETA: 0s - loss: 0.8212 - perplexity: 2.2733 - accuracy: 0.8548\n",
            "Sample text:\n",
            "yet complete not is he what knows who know to me to you are you ” “why you  “do cried now” you do why why  you if what ” “what ”...\n",
            "\n",
            "634/634 [==============================] - 136s 215ms/step - loss: 0.8212 - perplexity: 2.2732 - accuracy: 0.8548 - val_loss: 1.1291 - val_perplexity: 3.0929 - val_accuracy: 0.8507 - lr: 0.0010\n",
            "Epoch 8/10\n",
            "633/634 [============================>.] - ETA: 0s - loss: 0.8046 - perplexity: 2.2357 - accuracy: 0.8555\n",
            "Sample text:\n",
            "yet complete not is that it   that say you do i know to wish you do “why a have    i but me with angry not was i and...\n",
            "\n",
            "634/634 [==============================] - 136s 215ms/step - loss: 0.8046 - perplexity: 2.2357 - accuracy: 0.8555 - val_loss: 1.1459 - val_perplexity: 3.1453 - val_accuracy: 0.8505 - lr: 0.0010\n",
            "Epoch 9/10\n",
            "633/634 [============================>.] - ETA: 0s - loss: 0.7911 - perplexity: 2.2058 - accuracy: 0.8563\n",
            "Sample text:\n",
            "yet complete not is she what know you  do what to wish not does she that and you do  “what prince the said he that rays” his from rose prince ”...\n",
            "\n",
            "634/634 [==============================] - 136s 215ms/step - loss: 0.7911 - perplexity: 2.2059 - accuracy: 0.8563 - val_loss: 1.1594 - val_perplexity: 3.1882 - val_accuracy: 0.8504 - lr: 0.0010\n",
            "Epoch 10/10\n",
            "633/634 [============================>.] - ETA: 0s - loss: 0.7812 - perplexity: 2.1840 - accuracy: 0.8570\n",
            "Sample text:\n",
            "yet complete not is there  course “of drily lebedeff said prince” dear “my said he wished—he ” “why said pavlovitch’s” “evgenie   and century the sun’s to  as  just...\n",
            "\n",
            "634/634 [==============================] - 136s 214ms/step - loss: 0.7812 - perplexity: 2.1841 - accuracy: 0.8570 - val_loss: 1.1700 - val_perplexity: 3.2220 - val_accuracy: 0.8503 - lr: 0.0010\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU_IdIed2kqD",
        "outputId": "7cb84303-7c41-418b-c5f0-a616ad21aa21"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "def sample_token(logits):\r\n",
        "        logits, indices = tf.math.top_k(logits, k=5, sorted=True)\r\n",
        "        indices = np.asarray(indices).astype(\"int32\")\r\n",
        "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\r\n",
        "        preds = np.asarray(preds).astype(\"float32\")\r\n",
        "        return np.random.choice(indices, p=preds)\r\n",
        "\r\n",
        "def generate_text(prompt, response_length=20):\r\n",
        "    decoded_sample = prompt\r\n",
        "    for i in range(response_length-1):\r\n",
        "        tokenized_prompt = vectorize_layer([decoded_sample])[:, :-1]\r\n",
        "        predictions = model.predict([tokenized_prompt], verbose=0)\r\n",
        "        sample_index = len(decoded_sample.strip().split())-1\r\n",
        "\r\n",
        "        sampled_token = sample_token(predictions[0][sample_index])\r\n",
        "        sampled_token = index_lookup[sampled_token]\r\n",
        "        decoded_sample += \" \" + sampled_token\r\n",
        "    return decoded_sample\r\n",
        "\r\n",
        "\r\n",
        "def invert_sentence(sentence):\r\n",
        "  return \" \".join(reversed(sentence.split(' ')))\r\n",
        "\r\n",
        "model.save('saved_model/my_model')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as token_embedding2_layer_call_fn, token_embedding2_layer_call_and_return_conditional_losses, position_embedding2_layer_call_fn, position_embedding2_layer_call_and_return_conditional_losses, multi_head_attention_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "metadata": {
        "id": "RxClb-s37lfV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b94b3739-c910-4528-a310-7097ca11c5b4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "truth = generate_text('truth', 10)\r\n",
        "sky = generate_text('sky', 10)\r\n",
        "\r\n",
        "print(invert_sentence(truth))\r\n",
        "print(invert_sentence(sky))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "more and there was a foolish presentiment of the truth\n",
            "with her insanity” said “a firm resounded in the sky\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtPCaw3z7tkn",
        "outputId": "1e57c40b-1571-41ff-89df-237233ac6a7a"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}