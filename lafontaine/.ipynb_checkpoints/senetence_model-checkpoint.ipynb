{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gutenbergpy in /usr/local/lib/python3.8/dist-packages (0.3.4)\n",
      "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.8/dist-packages (from gutenbergpy) (0.18.2)\n",
      "Requirement already satisfied: httpsproxy-urllib2 in /usr/local/lib/python3.8/dist-packages (from gutenbergpy) (1.0)\n",
      "Requirement already satisfied: chardet in /usr/lib/python3/dist-packages (from gutenbergpy) (3.0.4)\n",
      "Requirement already satisfied: lxml>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from gutenbergpy) (4.9.1)\n",
      "Requirement already satisfied: pymongo in /usr/local/lib/python3.8/dist-packages (from gutenbergpy) (4.3.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from gutenbergpy) (65.3.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from pymongo->gutenbergpy) (2.2.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Cache already exists\n",
      "0 / 29\n",
      "1 / 29\n",
      "2 / 29\n",
      "3 / 29\n",
      "4 / 29\n",
      "5 / 29\n",
      "6 / 29\n",
      "7 / 29\n",
      "8 / 29\n",
      "9 / 29\n",
      "10 / 29\n",
      "11 / 29\n",
      "12 / 29\n",
      "13 / 29\n",
      "14 / 29\n",
      "15 / 29\n",
      "16 / 29\n",
      "17 / 29\n",
      "18 / 29\n",
      "19 / 29\n",
      "20 / 29\n",
      "21 / 29\n",
      "22 / 29\n",
      "23 / 29\n",
      "24 / 29\n",
      "25 / 29\n",
      "26 / 29\n",
      "27 / 29\n",
      "28 / 29\n"
     ]
    }
   ],
   "source": [
    "# We download our dataset for the training\n",
    "\n",
    "!pip3 install gutenbergpy\n",
    "import datasource\n",
    "datasource.download_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras_nlp\n",
      "  Downloading keras_nlp-0.3.1-py3-none-any.whl (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 9.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from keras_nlp) (1.23.2)\n",
      "Collecting tensorflow-text; platform_system != \"Darwin\"\n",
      "  Downloading tensorflow_text-2.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.9 MB 21.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from keras_nlp) (21.3)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from keras_nlp) (1.2.0)\n",
      "Requirement already satisfied: tensorflow; platform_system != \"Darwin\" in /usr/local/lib/python3.8/dist-packages (from keras_nlp) (2.10.0)\n",
      "Collecting tensorflow-hub>=0.8.0\n",
      "  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "\u001b[K     |████████████████████████████████| 108 kB 82.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->keras_nlp) (3.0.9)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow; platform_system != \"Darwin\"->keras_nlp) (1.14.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow; platform_system != \"Darwin\"->keras_nlp) (4.3.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow; platform_system != \"Darwin\"->keras_nlp) (1.1.2)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow; platform_system != \"Darwin\"->keras_nlp) (2.10.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow; platform_system != \"Darwin\"->keras_nlp) (3.3.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow; platform_system != \"Darwin\"->keras_nlp) (65.3.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in /usr/local/lib/python3.8/dist-packages (from tensorflow; platform_system != \"Darwin\"->keras_nlp) (2.10.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow; platform_system != \"Darwin\"->keras_nlp) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow; platform_system != \"Darwin\"->keras_nlp) (2.0.7)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow; platform_system != \"Darwin\"->keras_nlp) (3.19.4)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow; platform_system != \"Darwin\"->keras_nlp) (0.26.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow; platform_system != \"Darwin\"->keras_nlp) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow; platform_system != \"Darwin\"->keras_nlp) (14.0.6)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow; platform_system != \"Darwin\"->keras_nlp) (1.1.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow; platform_system != \"Darwin\"->keras_nlp) (1.48.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow; platform_system != \"Darwin\"->keras_nlp) (3.7.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow; platform_system != \"Darwin\"->keras_nlp) (0.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow; platform_system != \"Darwin\"->keras_nlp) (1.14.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow; platform_system != \"Darwin\"->keras_nlp) (2.10.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow; platform_system != \"Darwin\"->keras_nlp) (3.4.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow; platform_system != \"Darwin\"->keras_nlp) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow; platform_system != \"Darwin\"->keras_nlp) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow; platform_system != \"Darwin\"->keras_nlp) (2.11.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<2.11,>=2.10->tensorflow; platform_system != \"Darwin\"->keras_nlp) (2.22.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow; platform_system != \"Darwin\"->keras_nlp) (0.6.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard<2.11,>=2.10->tensorflow; platform_system != \"Darwin\"->keras_nlp) (0.34.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow; platform_system != \"Darwin\"->keras_nlp) (2.2.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow; platform_system != \"Darwin\"->keras_nlp) (4.12.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow; platform_system != \"Darwin\"->keras_nlp) (1.3.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow; platform_system != \"Darwin\"->keras_nlp) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow; platform_system != \"Darwin\"->keras_nlp) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow; platform_system != \"Darwin\"->keras_nlp) (4.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow; platform_system != \"Darwin\"->keras_nlp) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow; platform_system != \"Darwin\"->keras_nlp) (3.8.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow; platform_system != \"Darwin\"->keras_nlp) (3.2.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow; platform_system != \"Darwin\"->keras_nlp) (0.4.8)\n",
      "Installing collected packages: tensorflow-hub, tensorflow-text, keras-nlp\n",
      "Successfully installed keras-nlp-0.3.1 tensorflow-hub-0.12.0 tensorflow-text-2.10.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install keras_nlp\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import keras_nlp\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from pathlib import Path\n",
    "from vectorization import nw_split\n",
    "\n",
    "APP_DIR = Path.home() / \".lafontaine\"\n",
    "DATASET = APP_DIR / \"dataset.txt\"\n",
    "VECTORIZER = APP_DIR / \"vectorizer.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with VECTORIZER.open('rb') as stream:\n",
    "    from_disk = pickle.load(stream)\n",
    "\n",
    "vectorizer = TextVectorization.from_config(from_disk['config'])\n",
    "vectorizer.set_weights(from_disk['weights'])\n",
    "\n",
    "seq_length = from_disk['seq_length']\n",
    "\n",
    "with DATASET.open('r') as stream:\n",
    "    verses = stream.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(verses)\n",
    "length = len(verses)\n",
    "text_train = verses[:int(0.7*length)]\n",
    "text_test = verses[int(0.7*length):int(0.85*length)]\n",
    "text_valid = verses[int(0.85*length):]\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(text_train)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=256)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(text_test)\n",
    "test_dataset = test_dataset.shuffle(buffer_size=256)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices(text_valid)\n",
    "valid_dataset = valid_dataset.shuffle(buffer_size=256)\n",
    "valid_dataset = valid_dataset.batch(batch_size)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorizer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_text)\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset = test_dataset.map(preprocess_text)\n",
    "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "valid_dataset = valid_dataset.map(preprocess_text)\n",
    "valid_dataset = valid_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "vocab_size = len(vocab)\n",
    "index_lookup = dict(zip(range(len(vocab)), vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, 50, 128)          3756672   \n",
      " g (TokenAndPositionEmbeddin                                     \n",
      " g)                                                              \n",
      "                                                                 \n",
      " transformer_decoder (Transf  (None, 50, 128)          99584     \n",
      " ormerDecoder)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50, 29299)         3779571   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,635,827\n",
      "Trainable params: 7,635,827\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "\n",
    "def create_model():\n",
    "    inputs = keras.layers.Input(shape=(seq_length,), dtype=tf.int32)\n",
    "    embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(vocab_size, seq_length, embed_dim)(inputs)\n",
    "    decoder = keras_nlp.layers.TransformerDecoder(intermediate_dim=embed_dim, \n",
    "                                                            num_heads=num_heads, \n",
    "                                                            dropout=0.5)(embedding_layer)\n",
    "    \n",
    "    outputs = keras.layers.Dense(vocab_size, activation='softmax')(decoder)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=\"adam\", \n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=[keras_nlp.metrics.Perplexity(), 'accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSampler(keras.callbacks.Callback):\n",
    "    def __init__(self, start_prompt, max_tokens):\n",
    "        self.start_prompt = start_prompt\n",
    "        self.max_tokens = max_tokens\n",
    "        \n",
    "    # Helper method to choose a word from the top K probable words with respect to their probabilities\n",
    "    # in a sequence\n",
    "    def sample_token(self, logits):\n",
    "        logits, indices = tf.math.top_k(logits, k=5, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        decoded_sample = self.start_prompt\n",
    "        \n",
    "        for i in range(self.max_tokens-1):\n",
    "            tokenized_prompt = vectorizer([decoded_sample])[:, :-1]\n",
    "            predictions = self.model.predict([tokenized_prompt], verbose=0)\n",
    "            # To find the index of the next word in the prediction array.\n",
    "            # The tokenized prompt is already shorter than the original decoded sample\n",
    "            # by one, len(decoded_sample.split()) is two words ahead - so we remove 1 to get\n",
    "            # the next word in the sequence\n",
    "            sample_index = len(decoded_sample.strip().split())-1\n",
    "            \n",
    "            sampled_token = self.sample_token(predictions[0][sample_index])\n",
    "            sampled_token = index_lookup[sampled_token]\n",
    "            decoded_sample += \" \" + sampled_token\n",
    "            \n",
    "        print(f\"\\nSample text:\\n{decoded_sample}...\\n\")\n",
    "\n",
    "# First 5 words of a random sentence to be used as a seed\n",
    "random_sentence = ' '.join(random.choice(text_valid).replace('\\n', ' ').split(' ')[:4])\n",
    "sampler = TextSampler(random_sentence, 30)\n",
    "reducelr = keras.callbacks.ReduceLROnPlateau(patience=10, monitor='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "682/682 [==============================] - ETA: 0s - loss: 1.4969 - perplexity: 4.4680 - accuracy: 0.8271"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "history = model.fit(train_dataset, \n",
    "                    validation_data=valid_dataset,\n",
    "                    epochs=10, \n",
    "                    callbacks=[sampler, reducelr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_token(logits):\n",
    "        logits, indices = tf.math.top_k(logits, k=5, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "def generate_text(prompt, response_length=20):\n",
    "    decoded_sample = prompt\n",
    "    for i in range(response_length-1):\n",
    "        tokenized_prompt = vectorizer([decoded_sample])[:, :-1]\n",
    "        predictions = model.predict([tokenized_prompt], verbose=0)\n",
    "        sample_index = len(decoded_sample.strip().split())-1\n",
    "\n",
    "        sampled_token = sample_token(predictions[0][sample_index])\n",
    "        sampled_token = index_lookup[sampled_token]\n",
    "        decoded_sample += \" \" + sampled_token\n",
    "    return decoded_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"   é s ' un soir de ses doigts de l ’ est pas .   _\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
